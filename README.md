# yt-channel-expert (local LLM YouTube Channel Expert) — Monorepo Spec + Python Core

This repository is a **standalone Python package** that implements the *backend AI business logic* for a
**local-first “YouTube Channel Expert”**:

- Builds an offline **Channel Pack** from a channel’s videos (metadata + transcripts you are authorized to use).
- Supports **long-form videos (1–2+ hours)** using **hierarchical indexing** (micro-chunks → sections → episode summary).
- Answers questions with a **RAG pipeline** and produces **timestamped citations** (video + time ranges).
- Designed so the same **Channel Pack** can be consumed by:
  - this Python runtime (macOS “headless” / CLI), and
  - a future iOS client (Swift) that runs inference locally (MLC/llama.cpp), using the same pack format.

**No UI included**. Use the CLI (`ytce`) and the Python API.

## Quickstart (developer)

```bash
python -m venv .venv
source .venv/bin/activate
pip install -e ".[dev]"

# Build a pack from local transcript files + a videos manifest (JSON)
ytce pack build --input examples/demo_channel --out packs/demo_channel.pack

# Ask questions against the pack (uses a mock LLM unless you configure a local LLM backend)
ytce pack ask --pack packs/demo_channel.pack --question "What are the creator's recurring themes?"
```

## Key docs

Start here:
- `docs/00_overview.md`
- `docs/02_architecture.md`
- `docs/06_embeddings_and_index.md`
- `docs/07_rag_answering.md`
- `docs/08_channel_pack_format.md`

## Safety / compliance note

This project is designed to work with transcripts and audio that you are authorized to use.
It intentionally does **not** include scraping instructions for protected caption endpoints.

---

Generated by ChatGPT as a design-spec + scaffold. Replace the toy components (hash embeddings, mock LLM)
with production backends as you implement.

## Using `llmhub` (recommended)

This repo can use either:
- direct local runtimes (`llama.cpp`, `MLX`), or
- your unified provider package **`llmhub`**.

### Configure llmhub backend

Create a JSON config like:

```json
{
  "llm": {
    "backend": "llmhub",
    "provider": "openai",
    "model": "gpt-4.1-mini",
    "max_new_tokens": 512,
    "temperature": 0.2,
    "extra": {
      "api_key_env": "OPENAI_API_KEY",
      "timeout_s": 60
    }
  }
}
```

Then run:

```bash
ytce pack ask --pack packs/demo_channel.pack --question "What is emphasized?" --config config.json
```

**Note:** `LLMHubBackend` is an adapter and may need minor edits to match your exact `llmhub` public API.
See `src/yt_channel_expert/llm/llmhub_backend.py`.

## Using llmhub-node (recommended)

This repo can route generation to local models or API providers via **llmhub-node**.

### Start the llmhub server

A minimal server is included:

```bash
cd services/llmhub_server
npm install
export OPENAI_API_KEY=...
npm run dev
```

### Configure Python to call llmhub

Create `config.json`:

```json
{
  "llm": {
    "backend": "llmhub",
    "provider": "openai",
    "model": "gpt-4o",
    "temperature": 0.2,
    "max_new_tokens": 600,
    "extra": { "base_url": "http://localhost:8787", "timeout_s": 90 }
  }
}
```

Then:

```bash
ytce pack ask --pack packs/demo_channel.pack --question "What does the creator emphasize?" --config config.json
```

### Streaming

```bash
ytce pack ask --pack packs/demo_channel.pack --question "Summarize episode 2" --config config.json --stream
```

See `docs/13_llmhub_integration.md`.
